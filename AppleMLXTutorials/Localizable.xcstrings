{
  "sourceLanguage" : "en",
  "strings" : {
    "**1. LoRA Configuration and Application:**" : {
      "comment" : "A heading for the first section of code examples in Chapter 13.",
      "isCommentAutoGenerated" : true
    },
    "**2. Prepare Training Data (JSONL format):**" : {
      "comment" : "A heading for the section where training data is prepared in JSONL format.",
      "isCommentAutoGenerated" : true
    },
    "**3. LoRA Training:**" : {
      "comment" : "A heading for the section of the code example related to training the LoRA model.",
      "isCommentAutoGenerated" : true
    },
    "**4. LoRA Evaluation:**" : {
      "comment" : "A heading for evaluating a LoRA model.",
      "isCommentAutoGenerated" : true
    },
    "**Activation Functions:**" : {
      "comment" : "A heading for the section on activation functions.",
      "isCommentAutoGenerated" : true
    },
    "**Activation functions** and **loss functions** are core components of neural networks.\n\n**Activation Functions:**\nAdd non-linearity to enable neural networks to learn complex patterns.\n• **ReLU** - max(0, x), most commonly used\n• **GELU** - Primarily used in Transformers\n• **SiLU/Swish** - x * sigmoid(x)\n• **Sigmoid** - Compresses output to 0~1\n• **Softmax** - Converts to probability distribution\n\n**Loss Functions:**\nMeasure the difference between model predictions and actual values.\n• **MSE** - Regression problems\n• **Cross-Entropy** - Classification problems\n• **Binary Cross-Entropy** - Binary classification\n\n**Loss Function Selection Guide:**\n• Regression → MSE, MAE\n• Binary classification → Binary Cross-Entropy\n• Multi-class classification → Cross-Entropy (with Softmax)" : {
      "comment" : "An explanation of activation functions and loss functions in neural networks.",
      "isCommentAutoGenerated" : true
    },
    "**Adam Optimizer:**" : {
      "comment" : "An introductory text for the Adam optimizer section.",
      "isCommentAutoGenerated" : true
    },
    "**Add to Package.swift:**" : {

    },
    "**Arithmetic Operations:**" : {

    },
    "**Array Creation Methods:**" : {

    },
    "**Array Properties:**" : {
      "comment" : "A heading for the properties of an MLXArray.",
      "isCommentAutoGenerated" : true
    },
    "**Automatic Differentiation** is at the core of deep learning.\nMLX automatically computes gradients through function transforms.\n\n**Key Concepts:**\n• **grad** - Returns a function that computes the gradient (derivative)\n• **valueAndGrad** - Returns both function value and gradient simultaneously\n• **Backpropagation** - Gradient propagation from output to input\n\n**Use Cases:**\n• Computing gradients of loss functions\n• Updating neural network weights\n• Implementing optimization algorithms\n\n**Mathematical Background:**\nIf f(x) = x², then\nf'(x) = 2x\n\nMLX's grad computes this automatically." : {
      "comment" : "An explanatory text about automatic differentiation in the context of deep learning.",
      "isCommentAutoGenerated" : true
    },
    "**Basic Gradient Computation:**" : {
      "comment" : "A heading for an example of basic gradient computation in SwiftUI.",
      "isCommentAutoGenerated" : true
    },
    "**Basic Optimizer Usage:**" : {
      "comment" : "A heading for the basic usage of an optimizer.",
      "isCommentAutoGenerated" : true
    },
    "**Basic Usage:**" : {
      "comment" : "A heading indicating that basic usage examples are provided below.",
      "isCommentAutoGenerated" : true
    },
    "**Computing Value and Gradient Simultaneously:**" : {

    },
    "**Define MLP Model:**" : {
      "comment" : "A code block header indicating the definition of a Multi-Layer Perceptron (MLP) model.",
      "isCommentAutoGenerated" : true
    },
    "**Device Checking and Selection:**" : {
      "comment" : "A heading for an example of checking and selecting devices in SwiftUI.",
      "isCommentAutoGenerated" : true
    },
    "**Embedding Layer:**" : {
      "comment" : "A heading for an example of an embedding layer in SwiftUI.",
      "isCommentAutoGenerated" : true
    },
    "**Full Training Loop:**" : {
      "comment" : "A heading for the full training loop code example.",
      "isCommentAutoGenerated" : true
    },
    "**Gradients of Multivariable Functions:**" : {
      "comment" : "A heading for an example of computing gradients for functions with multiple inputs.",
      "isCommentAutoGenerated" : true
    },
    "**Linear Layer:**" : {
      "comment" : "A code block header for a linear layer definition.",
      "isCommentAutoGenerated" : true
    },
    "**Load Model Parameters:**" : {
      "comment" : "A heading for the code example that demonstrates loading model parameters from a file.",
      "isCommentAutoGenerated" : true
    },
    "**LoRA (Low-Rank Adaptation)** is a technique for efficiently fine-tuning large models.\n\n**LoRA Principle:**\n- Learn only low-rank matrices instead of all weights\n- W' = W + BA (B: d x r, A: r x k, r << min(d,k))\n- Original model weights remain frozen\n\n**Advantages:**\n- **Memory Efficient** - 1/10 memory of full fine-tuning\n- **Fast Training** - Significantly fewer trainable parameters\n- **Model Merging** - Can combine multiple LoRAs\n- **Storage** - Only save LoRA adapter (few MBs)\n\n**Use Cases:**\n- Domain specialization (medical, legal, finance)\n- Style adjustment (formal, casual)\n- Language adaptation (Korean enhancement)\n- Specific tasks (code generation, summarization)" : {
      "comment" : "An explanation of LoRA, its principle, advantages, and use cases.",
      "isCommentAutoGenerated" : true
    },
    "**Loss Function:**" : {
      "comment" : "A heading for the loss function section in the code example.",
      "isCommentAutoGenerated" : true
    },
    "**Loss Functions and Gradients:**" : {
      "comment" : "A section header explaining loss functions and their gradients.",
      "isCommentAutoGenerated" : true
    },
    "**Loss Functions:**" : {
      "comment" : "An introductory text in the code section of the view, introducing the concept of loss functions.",
      "isCommentAutoGenerated" : true
    },
    "**Math Functions:**" : {

    },
    "**Memory Management:**" : {
      "comment" : "A heading in the code section of the Chapter 4 view.",
      "isCommentAutoGenerated" : true
    },
    "**MLX** is an array framework designed for machine learning on Apple Silicon.\nDeveloped by Apple's machine learning research team, it provides a NumPy-like API.\n\n**Key Features:**\n• **Unified Memory Model** - CPU and GPU share memory, no data copying needed\n• **Lazy Evaluation** - Arrays are computed only when needed\n• **Automatic Differentiation** - Automatic gradient computation through function transforms\n• **Multi-device** - Flexible computation on CPU and GPU\n\n**MLX Swift Package Components:**\n• `MLX` - Core array framework\n• `MLXNN` - Neural network modules\n• `MLXOptimizers` - Optimization algorithms\n• `MLXRandom` - Random number generation\n\n**Supported Platforms:**\n• macOS 14.0+ (Apple Silicon)\n• iOS 17.0+ (Apple Silicon)" : {

    },
    "**MLXNN** provides high-level modules for building neural networks.\nIt has an API similar to PyTorch's torch.nn.\n\n**Core Components:**\n• **Module** - Base class for all neural network layers\n• **Linear** - Fully connected layer\n• **Embedding** - Embedding layer\n• **Conv2d** - 2D convolution layer\n\n**Module Protocol:**\n• `callAsFunction(_:)` - Performs forward pass\n• `parameters()` - Returns trainable parameters\n• `update(parameters:)` - Updates parameters\n\n**Layer Types:**\n• Linear layers: Linear\n• Activations: ReLU, GELU, SiLU, Sigmoid\n• Normalization: LayerNorm, RMSNorm, BatchNorm\n• Attention: MultiHeadAttention" : {
      "comment" : "An overview of the key features of MLXNN.",
      "isCommentAutoGenerated" : true
    },
    "**Optimizers** use gradients to update model parameters.\n\n**Optimizers provided by MLXOptimizers:**\n• **SGD** - Stochastic Gradient Descent\n• **Adam** - Most widely used optimizer\n• **AdamW** - Adam with Weight Decay\n• **Adagrad** - Adaptive learning rate\n• **RMSprop** - Moving average based\n\n**Key Concepts:**\n• **Learning Rate** - Controls update magnitude\n• **Momentum** - Maintains previous gradient direction\n• **Weight Decay** - Regularization to prevent overfitting\n\n**Learning Rate Scheduling:**\n• Fixed learning rate\n• Step decay\n• Cosine annealing\n• Warmup + decay" : {
      "comment" : "An overview of optimizers in SwiftUI.",
      "isCommentAutoGenerated" : true
    },
    "**Reduce Operations:**" : {
      "comment" : "The title for a section of code that demonstrates reduce operations on `MLXArray`.",
      "isCommentAutoGenerated" : true
    },
    "**Save Checkpoint:**" : {
      "comment" : "A heading for a section of code that demonstrates saving a training checkpoint.",
      "isCommentAutoGenerated" : true
    },
    "**Save Model Parameters:**" : {
      "comment" : "An example of saving model parameters.",
      "isCommentAutoGenerated" : true
    },
    "**SDXL Turbo** - 빠른 이미지 생성 (~5GB)" : {
      "comment" : "An explanation of the \"SDXL Turbo\" model, highlighting its speed and size.",
      "isCommentAutoGenerated" : true
    },
    "**Simple MLP Model:**" : {
      "comment" : "A heading for a code example of a simple Multi-Layer Perceptron (MLP) model.",
      "isCommentAutoGenerated" : true
    },
    "**Stable Diffusion** is a diffusion model that generates images from text prompts.\n\n**What you can do in this chapter:**\n- Download SDXL Turbo model (~5GB)\n- Generate images from text prompts\n- Remove unwanted elements with Negative Prompt\n\n**Note:**\n- Model download required on first run (~5GB)\n- SDXL Turbo offers fast generation (2-4 steps)\n- Minimum 8GB RAM recommended" : {
      "comment" : "An overview of the chapter, including the model download, image generation, and the use of Negative Prompt.",
      "isCommentAutoGenerated" : true
    },
    "**Streams and Synchronization:**" : {
      "comment" : "The heading for the section on streams and synchronization in the code example.",
      "isCommentAutoGenerated" : true
    },
    "**Training Loop:**" : {
      "comment" : "An introductory text about the training loop in the code example.",
      "isCommentAutoGenerated" : true
    },
    "**Vision Language Models (VLM)** are multimodal models that understand both images and text together.\n\n**What you can do in this chapter:**\n- Download and load VLM models\n- Upload and analyze images\n- Ask questions about images\n\n**Use cases:**\n- Generate photo descriptions\n- Recognize objects in images\n- Analyze documents/charts" : {
      "comment" : "An overview of Vision Language Models (VLM).",
      "isCommentAutoGenerated" : true
    },
    "%@" : {
      "comment" : "A circular badge with a number and a title and description. The first argument is the number to display in the badge. The second argument is the title to display above the badge. The third argument is the description to",
      "isCommentAutoGenerated" : true
    },
    "%@%%" : {
      "comment" : "A text displaying the percentage of the training process that has been completed.",
      "isCommentAutoGenerated" : true
    },
    "%@%% Downloading..." : {
      "comment" : "A text indicating the progress of downloading a model. The value shown is the percentage of the download that has completed.",
      "isCommentAutoGenerated" : true
    },
    "`MLXArray` is the core data structure of MLX.\nSimilar to NumPy's ndarray, it represents multi-dimensional arrays.\n\n**Key Features:**\n• Support for various data types (Float32, Int32, Bool, etc.)\n• Multi-dimensional arrays (scalars, vectors, matrices, tensors)\n• Lazy evaluation - actual computation performed when `eval()` is called\n• Broadcasting support\n\n**Data Types:**\n• `.float32`, `.float16`, `.bfloat16` - Floating point\n• `.int32`, `.int64`, `.uint32` - Integers\n• `.bool` - Boolean\n• `.complex64` - Complex numbers" : {
      "comment" : "An explanatory text describing the purpose and features of `MLXArray`.",
      "isCommentAutoGenerated" : true
    },
    "1. Model Download" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "1. 모델 다운로드"
          }
        }
      }
    },
    "1. Model Selection & Download" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "1. 모델 선택 및 다운로드"
          }
        }
      }
    },
    "2. Prompt Input" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "2. 프롬프트 입력"
          }
        }
      }
    },
    "3. Generated Result" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "3. 생성 결과"
          }
        }
      }
    },
    "512 × 512 pixels" : {
      "comment" : "The size of the generated image in pixels.",
      "isCommentAutoGenerated" : true
    },
    "Abstract" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "추상"
          }
        }
      }
    },
    "Analyze" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "분석"
          }
        }
      }
    },
    "Analyzing..." : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "분석 중..."
          }
        }
      }
    },
    "by " : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "by "
          }
        }
      }
    },
    "Chapter %@" : {
      "comment" : "A label displaying the title and subtitle of a chapter.",
      "isCommentAutoGenerated" : true
    },
    "Chapter %@: %@" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "new",
            "value" : "Chapter %1$@: %2$@"
          }
        }
      }
    },
    "Chapter %lld" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Chapter %lld"
          }
        }
      }
    },
    "chapter.1.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Introduction to MLX framework and machine learning on Apple Silicon."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLX 프레임워크 소개와 Apple Silicon에서의 머신러닝을 알아봅니다."
          }
        }
      }
    },
    "chapter.1.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Introduction"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "소개"
          }
        }
      }
    },
    "chapter.1.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLX Introduction"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLX 소개"
          }
        }
      }
    },
    "chapter.2.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Create arrays and perform basic operations using MLXArray."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXArray를 사용하여 배열을 생성하고 기본 연산을 수행합니다."
          }
        }
      }
    },
    "chapter.2.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXArray Basics"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXArray 기초"
          }
        }
      }
    },
    "chapter.2.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Array Basics"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "배열 기초"
          }
        }
      }
    },
    "chapter.3.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Learn math operations, broadcasting, and reduce operations."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "수학 연산, 브로드캐스팅, 리듀스 연산을 학습합니다."
          }
        }
      }
    },
    "chapter.3.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Array Operations"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "배열 연산"
          }
        }
      }
    },
    "chapter.3.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Array Operations"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "배열 연산"
          }
        }
      }
    },
    "chapter.4.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Understand CPU/GPU operations and unified memory model."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "CPU와 GPU 간 연산 및 통합 메모리 모델을 이해합니다."
          }
        }
      }
    },
    "chapter.4.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Device Management"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "디바이스 관리"
          }
        }
      }
    },
    "chapter.4.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Device Management"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "디바이스 관리"
          }
        }
      }
    },
    "chapter.5.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Learn automatic differentiation and backpropagation using grad function."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "grad 함수를 사용한 자동 미분과 역전파를 학습합니다."
          }
        }
      }
    },
    "chapter.5.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Automatic Differentiation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "자동 미분"
          }
        }
      }
    },
    "chapter.5.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Automatic Differentiation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "자동 미분"
          }
        }
      }
    },
    "chapter.6.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Build basic neural network layers using MLXNN module."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXNN 모듈을 사용하여 기본 신경망 레이어를 구성합니다."
          }
        }
      }
    },
    "chapter.6.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXNN Basics"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLXNN 기초"
          }
        }
      }
    },
    "chapter.6.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Neural Network Basics"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "신경망 기초"
          }
        }
      }
    },
    "chapter.7.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Learn activation functions like ReLU, Softmax and loss functions."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "ReLU, Softmax 등 활성화 함수와 손실 함수를 학습합니다."
          }
        }
      }
    },
    "chapter.7.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Activation & Loss"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "활성화 & 손실"
          }
        }
      }
    },
    "chapter.7.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Activation & Loss"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "활성화 & 손실 함수"
          }
        }
      }
    },
    "chapter.8.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Train models using optimizers like SGD and Adam."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "SGD, Adam 등 옵티마이저를 사용하여 모델을 학습합니다."
          }
        }
      }
    },
    "chapter.8.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Optimizers"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "옵티마이저"
          }
        }
      }
    },
    "chapter.8.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Optimizers"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "옵티마이저"
          }
        }
      }
    },
    "chapter.9.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Save and load trained model weights."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "학습된 모델의 가중치를 저장하고 불러옵니다."
          }
        }
      }
    },
    "chapter.9.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Save & Load"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "저장 & 로드"
          }
        }
      }
    },
    "chapter.9.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Save & Load"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "모델 저장/로드"
          }
        }
      }
    },
    "chapter.10.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Practical example: Train an MNIST handwritten digit classifier."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "실전 예제: MNIST 손글씨 숫자 분류 모델을 학습합니다."
          }
        }
      }
    },
    "chapter.10.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MNIST Classifier"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MNIST 분류기"
          }
        }
      }
    },
    "chapter.10.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MNIST Classifier"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MNIST 분류기"
          }
        }
      }
    },
    "chapter.11.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Generate text using large language models with MLX-LM."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLX-LM을 사용하여 대형 언어 모델로 텍스트를 생성합니다."
          }
        }
      }
    },
    "chapter.11.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Text Generation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "텍스트 생성"
          }
        }
      }
    },
    "chapter.11.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM Text Generation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM 텍스트 생성"
          }
        }
      }
    },
    "chapter.12.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Analyze images using vision language models."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "비전 언어 모델을 사용하여 이미지를 분석합니다."
          }
        }
      }
    },
    "chapter.12.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Vision Language Model"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "비전 언어 모델"
          }
        }
      }
    },
    "chapter.12.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "VLM Image Analysis"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "VLM 이미지 분석"
          }
        }
      }
    },
    "chapter.13.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Efficiently fine-tune models using LoRA."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LoRA를 사용하여 모델을 효율적으로 파인튜닝합니다."
          }
        }
      }
    },
    "chapter.13.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Low-Rank Adaptation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Low-Rank Adaptation"
          }
        }
      }
    },
    "chapter.13.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LoRA Fine-tuning"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LoRA 파인튜닝"
          }
        }
      }
    },
    "chapter.14.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Generate images using Stable Diffusion."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Stable Diffusion을 사용하여 이미지를 생성합니다."
          }
        }
      }
    },
    "chapter.14.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Stable Diffusion"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Stable Diffusion"
          }
        }
      }
    },
    "chapter.14.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Image Generation"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "이미지 생성"
          }
        }
      }
    },
    "chapter.15.description" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Explore MLX ecosystem and additional learning resources."
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "MLX 생태계와 추가 학습 리소스를 안내합니다."
          }
        }
      }
    },
    "chapter.15.subtitle" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ecosystem & Next Steps"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "생태계 & 다음 단계"
          }
        }
      }
    },
    "chapter.15.title" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ecosystem & Next Steps"
          }
        },
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "생태계 & 다음 단계"
          }
        }
      }
    },
    "Clear" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "지우기"
          }
        }
      }
    },
    "Code Example" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "코드 예제"
          }
        }
      }
    },
    "Code Request" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "코드 요청"
          }
        }
      }
    },
    "Copy" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "복사"
          }
        }
      }
    },
    "Creative Writing" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "창의적 글쓰기"
          }
        }
      }
    },
    "Cross-Platform Korea" : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "크로스플랫폼 코리아"
          }
        }
      }
    },
    "Download Model" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "모델 다운로드"
          }
        }
      }
    },
    "Downloading..." : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "다운로드 중..."
          }
        }
      }
    },
    "Enter a prompt and press 'Generate Image' button." : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "프롬프트를 입력하고 '이미지 생성' 버튼을 누르세요."
          }
        }
      }
    },
    "Enter a prompt and press 'Generate Text' button." : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "프롬프트를 입력하고 '텍스트 생성' 버튼을 누르세요."
          }
        }
      }
    },
    "Examples:" : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "예시:"
          }
        }
      }
    },
    "Generate Image" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "이미지 생성"
          }
        }
      }
    },
    "Generate Text" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "텍스트 생성"
          }
        }
      }
    },
    "Generating..." : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "생성 중..."
          }
        }
      }
    },
    "Korean Question" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "한국어 질문"
          }
        }
      }
    },
    "Landscape" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "풍경"
          }
        }
      }
    },
    "Machine Learning Tutorial" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "머신러닝 튜토리얼"
          }
        }
      }
    },
    "MLX is a unified framework for machine learning on Apple Silicon.\nIt supports both Python and Swift APIs and is actively developed." : {

    },
    "MLX is an open-source project and welcomes community contributions." : {

    },
    "MLX provides rich array operations similar to NumPy.\n\n**Arithmetic Operations:**\n• Addition, subtraction, multiplication, division\n• Element-wise operations\n• Broadcasting support\n\n**Math Functions:**\n• `exp`, `log`, `sqrt`, `abs`\n• `sin`, `cos`, `tan`\n• `power`, `square`\n\n**Reduce Operations:**\n• `sum`, `mean`, `prod`\n• `min`, `max`\n• `argMax`, `argMin`\n\n**Shape Transformations:**\n• `reshape` - Change shape\n• `transpose` - Transpose\n• `flatten` - Flatten to 1D" : {
      "comment" : "An introductory text about MLX's array operations.",
      "isCommentAutoGenerated" : true
    },
    "MNIST is a classic dataset for handwritten digit (0-9) classification.\nIn this chapter, we combine everything learned so far to train a real classification model.\n\n**Dataset:**\n• 28x28 pixel grayscale images\n• 60,000 training images\n• 10,000 test images\n• 10 classes (0-9)\n\n**Model Architecture:**\n• Input: 784 (28x28 flattened)\n• Hidden layers: 256 -> 128\n• Output: 10 (class probabilities)\n\n**Training Process:**\n1. Load and preprocess data\n2. Define model (MLP)\n3. Loss function (Cross-Entropy)\n4. Optimizer (Adam)\n5. Run training loop\n6. Evaluate and measure accuracy" : {
      "comment" : "An overview of the MNIST dataset and the model architecture used in this chapter.",
      "isCommentAutoGenerated" : true
    },
    "Model Ready" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "모델 준비 완료"
          }
        }
      }
    },
    "Negative Prompt (elements to exclude)" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Negative Prompt (제외할 요소)"
          }
        }
      }
    },
    "Negative Prompt (제외할 요소)" : {
      "comment" : "A label describing the text field for adding elements to exclude from the generated image.",
      "isCommentAutoGenerated" : true
    },
    "Note: The actual MNIST dataset needs to be downloaded separately. Here we demonstrate the training process with synthetic data." : {
      "comment" : "A note explaining that the actual MNIST dataset needs to be downloaded separately, and that this example demonstrates training with synthetic data.",
      "isCommentAutoGenerated" : true
    },
    "One of MLX's greatest advantages is the **unified memory model**.\nLeveraging Apple Silicon's unified memory architecture, CPU and GPU share memory.\n\n**Key Concepts:**\n• **Unified Memory** - No data copying needed between CPU and GPU\n• **Device Selection** - Choose CPU or GPU per operation\n• **Automatic Device** - GPU prioritized by default\n\n**Supported Devices:**\n• `.cpu` - Run operations on CPU\n• `.gpu` - Run operations on GPU (Metal)\n\n**Benefits:**\n• Large models can utilize entire system memory\n• No data transfer overhead\n• Improved memory efficiency compared to PyTorch/TensorFlow" : {
      "comment" : "An explanatory text about MLX's unified memory model.",
      "isCommentAutoGenerated" : true
    },
    "Overview" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "개요"
          }
        }
      }
    },
    "Please download the model" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "모델을 다운로드하세요"
          }
        }
      }
    },
    "Please download the model first." : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "먼저 위에서 모델을 다운로드하세요."
          }
        }
      }
    },
    "Portrait" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "초상화"
          }
        }
      }
    },
    "Press the Run button to see results." : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "실행 버튼을 눌러 결과를 확인하세요."
          }
        }
      }
    },
    "Press the Run button to start training." : {
      "comment" : "A message displayed when the \"Run\" button is not pressed yet.",
      "isCommentAutoGenerated" : true
    },
    "Prompt (describe desired image)" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Prompt (원하는 이미지 설명)"
          }
        }
      }
    },
    "Prompt (원하는 이미지 설명)" : {
      "comment" : "A label describing the text editor where the user inputs their image prompt.",
      "isCommentAutoGenerated" : true
    },
    "Replace Model" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "모델 교체"
          }
        }
      }
    },
    "Run" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "실행"
          }
        }
      }
    },
    "Run large language models on Apple Silicon using **MLXLLM**.\n\n**What you can do in this chapter:**\n- Auto-download models from Hugging Face\n- Generate AI text locally\n- Select and compare various models\n\n**Note:** Model download is required on first run (internet connection needed)." : {
      "comment" : "An overview of the chapter, including its purpose and the tasks it covers.",
      "isCommentAutoGenerated" : true
    },
    "Run Result" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "실행 결과"
          }
        }
      }
    },
    "Save" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "저장"
          }
        }
      }
    },
    "Saving and loading trained models is essential for real-world applications.\n\n**Save Formats:**\n• **safetensors** - Recommended, safe and fast\n• **npz** - NumPy compatible\n• **gguf** - Commonly used for LLM models\n\n**What to Save:**\n• Model parameters (weights, biases)\n• Optimizer state (momentum, etc.)\n• Training checkpoints\n\n**MLX Save API:**\n• `save(arrays:url:)` - Save as safetensors\n• `loadArrays(url:)` - Load from file\n• `Module.parameters()` - Parameter dictionary\n\n**Checkpoints:**\nSave models during training to resume later or\npreserve the best performing model." : {
      "comment" : "An overview of saving and loading models.",
      "isCommentAutoGenerated" : true
    },
    "Select a Chapter" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "챕터를 선택하세요"
          }
        }
      }
    },
    "Select a chapter from the sidebar to start learning." : {
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "왼쪽 사이드바에서 학습할 챕터를 선택해주세요."
          }
        }
      }
    },
    "Select an image and press the 'Analyze' button." : {

    },
    "Select Image" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "이미지 선택"
          }
        }
      }
    },
    "Step %@" : {
      "comment" : "A text displaying the current generation step. The argument is the current generation step number.",
      "isCommentAutoGenerated" : true
    },
    "Stop" : {
      "extractionState" : "stale",
      "localizations" : {
        "ko" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "중지"
          }
        }
      }
    },
    "This tutorial was created by Cross Platform Korea." : {
      "comment" : "A note at the bottom of the \"Community\" section, acknowledging the creation of the tutorial by Cross Platform Korea.",
      "isCommentAutoGenerated" : true
    },
    "To run LoRA fine-tuning:\n\n1. **Sufficient Memory**\n   • 7B model LoRA: ~16GB RAM\n   • 1-3B model LoRA: ~8GB RAM\n\n2. **Training Data**\n   • JSONL format required\n   • Minimum 100-1000 samples\n\n3. **Training Time**\n   • 200 iterations: ~10-30 min\n   • GPU acceleration required (Apple Silicon)\n\n**Official Example:**\nRefer to LoRATrainingExample in mlx-swift-examples" : {
      "comment" : "A description of the requirements for running LoRA fine-tuning.",
      "isCommentAutoGenerated" : true
    },
    "이 챕터는 아직 준비 중입니다." : {
      "comment" : "A description text shown when a user tries to access a chapter that is not yet available.",
      "isCommentAutoGenerated" : true
    }
  },
  "version" : "1.1"
}